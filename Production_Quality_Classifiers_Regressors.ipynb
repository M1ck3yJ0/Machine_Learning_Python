{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlW655xjDPFB"
   },
   "source": [
    "##DATASET CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRI0uwN4qyeQ",
    "outputId": "f686d12a-4550-48f0-e142-de7668165f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date_time  T_data_1_1  T_data_1_2  T_data_1_3  T_data_2_1  \\\n",
      "0 2015-01-04 00:05:00  271.745455  338.545455  267.218182  328.236364   \n",
      "1 2015-01-04 01:05:00  277.583333  300.366667  273.550000  320.483333   \n",
      "2 2015-01-04 02:05:00  273.600000  231.833333  266.800000  322.700000   \n",
      "3 2015-01-04 03:05:00  250.333333  227.033333  256.350000  326.583333   \n",
      "4 2015-01-04 04:05:00  240.400000  239.350000  249.250000  325.750000   \n",
      "\n",
      "   T_data_2_2  T_data_2_3  T_data_3_1  T_data_3_2  T_data_3_3  T_data_4_1  \\\n",
      "0  329.400000  345.472727  501.981818  499.400000  588.636364  320.490909   \n",
      "1  331.966667  355.450000  501.900000  501.650000  705.516667  330.233333   \n",
      "2  334.216667  347.133333  501.133333  500.366667  579.600000  341.550000   \n",
      "3  333.666667  317.716667  511.183333  498.116667  492.366667  345.350000   \n",
      "4  325.400000  310.500000  522.683333  498.966667  538.716667  341.283333   \n",
      "\n",
      "   T_data_4_2  T_data_4_3  T_data_5_1  T_data_5_2  T_data_5_3      H_data  \\\n",
      "0  362.181818  336.363636  232.236364  236.454545  242.454545  155.900000   \n",
      "1  387.133333  336.316667  231.850000  237.683333  236.516667  156.446500   \n",
      "2  398.683333  334.350000  237.016667  245.683333  231.966667  156.000167   \n",
      "3  395.066667  332.233333  248.850000  254.150000  244.783333  156.047000   \n",
      "4  379.883333  337.816667  260.000000  260.516667  248.550000  188.481667   \n",
      "\n",
      "   AH_data  quality  \n",
      "0     4.73      392  \n",
      "1     7.90      384  \n",
      "2     6.96      393  \n",
      "3     7.29      399  \n",
      "4     7.11      400  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We load the given datasets\n",
    "df1 = pd.read_csv('/content/data_X.csv')  # Per-minute data from sensors (temperature, moisture, thickness)\n",
    "df2 = pd.read_csv('/content/data_Y.csv')  # Hourly data from quality testing\n",
    "\n",
    "# We convert the date_time columns to datetime format\n",
    "df1['date_time'] = pd.to_datetime(df1['date_time'])\n",
    "df2['date_time'] = pd.to_datetime(df2['date_time'])\n",
    "\n",
    "# We filter for temperature data starting from one hour before the first quality measurement\n",
    "# This is to ensure that we only take the relevant sensor data for each quality score, considering that the product stays in the oven for 1 hour\n",
    "start_date = df2['date_time'].min() - pd.Timedelta(hours=1)\n",
    "df1 = df1[df1['date_time'] >= start_date]\n",
    "\n",
    "# We now we average the sensor data over the preceding hour for every quality measurement\n",
    "# This is to ensure that we consider the mean chamber temperature during the one hour that the product is in the oven\n",
    "# We do this by setting the date_time as index for df1 and calculating hourly means for all columns (from hh:00 to hh:59)\n",
    "# hh:00 to hh:59 is considered since the humidity is the same for these times for every set of 60 records (1 hour)\n",
    "# From the above, we infer that it is the same product/batch since it is only measured ONCE, before entering the oven\n",
    "# The 5-minute delay to quality measure (hh+1:05) is considered to be the time required for QC\n",
    "df1.set_index('date_time', inplace=True)\n",
    "df1_hourly = df1.resample('H').mean().reset_index()  # Aggregating all columns\n",
    "\n",
    "# We adjust the timestamp to match the quality reading time (hh+1:05)\n",
    "df1_hourly['date_time'] = df1_hourly['date_time'] + pd.Timedelta(hours=1, minutes=5)\n",
    "\n",
    "# We perform an inner join on date_time to align with quality data\n",
    "merged_df = pd.merge(df1_hourly, df2, on='date_time', how='inner')\n",
    "\n",
    "# We save the merged dataset for modeling\n",
    "merged_df.to_csv('data_merged_mean.csv', index=False)\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oP84l-_ZByMJ"
   },
   "source": [
    "##DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFVNnG7Dqbif",
    "outputId": "d71cf54b-39b0-445f-ecef-78f55e4d32a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date_time  T_data_1_1  T_data_1_2  T_data_1_3  T_data_2_1  \\\n",
      "0  2015-01-04 00:05:00  271.745455  338.545455  267.218182  328.236364   \n",
      "1  2015-01-04 01:05:00  277.583333  300.366667  273.550000  320.483333   \n",
      "2  2015-01-04 02:05:00  273.600000  231.833333  266.800000  322.700000   \n",
      "3  2015-01-04 03:05:00  250.333333  227.033333  256.350000  326.583333   \n",
      "4  2015-01-04 04:05:00  240.400000  239.350000  249.250000  325.750000   \n",
      "\n",
      "   T_data_2_2  T_data_2_3  T_data_3_1  T_data_3_2  T_data_3_3  T_data_4_1  \\\n",
      "0  329.400000  345.472727  501.981818  499.400000  588.636364  320.490909   \n",
      "1  331.966667  355.450000  501.900000  501.650000  705.516667  330.233333   \n",
      "2  334.216667  347.133333  501.133333  500.366667  579.600000  341.550000   \n",
      "3  333.666667  317.716667  511.183333  498.116667  492.366667  345.350000   \n",
      "4  325.400000  310.500000  522.683333  498.966667  538.716667  341.283333   \n",
      "\n",
      "   T_data_4_2  T_data_4_3  T_data_5_1  T_data_5_2  T_data_5_3      H_data  \\\n",
      "0  362.181818  336.363636  232.236364  236.454545  242.454545  155.900000   \n",
      "1  387.133333  336.316667  231.850000  237.683333  236.516667  156.446500   \n",
      "2  398.683333  334.350000  237.016667  245.683333  231.966667  156.000167   \n",
      "3  395.066667  332.233333  248.850000  254.150000  244.783333  156.047000   \n",
      "4  379.883333  337.816667  260.000000  260.516667  248.550000  188.481667   \n",
      "\n",
      "   AH_data  quality  \n",
      "0     4.73      392  \n",
      "1     7.90      384  \n",
      "2     6.96      393  \n",
      "3     7.29      399  \n",
      "4     7.11      400  \n",
      "(29184, 19)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29184 entries, 0 to 29183\n",
      "Data columns (total 19 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   date_time   29184 non-null  object \n",
      " 1   T_data_1_1  29184 non-null  float64\n",
      " 2   T_data_1_2  29184 non-null  float64\n",
      " 3   T_data_1_3  29184 non-null  float64\n",
      " 4   T_data_2_1  29184 non-null  float64\n",
      " 5   T_data_2_2  29184 non-null  float64\n",
      " 6   T_data_2_3  29184 non-null  float64\n",
      " 7   T_data_3_1  29184 non-null  float64\n",
      " 8   T_data_3_2  29184 non-null  float64\n",
      " 9   T_data_3_3  29184 non-null  float64\n",
      " 10  T_data_4_1  29184 non-null  float64\n",
      " 11  T_data_4_2  29184 non-null  float64\n",
      " 12  T_data_4_3  29184 non-null  float64\n",
      " 13  T_data_5_1  29184 non-null  float64\n",
      " 14  T_data_5_2  29184 non-null  float64\n",
      " 15  T_data_5_3  29184 non-null  float64\n",
      " 16  H_data      29184 non-null  float64\n",
      " 17  AH_data     29184 non-null  float64\n",
      " 18  quality     29184 non-null  int64  \n",
      "dtypes: float64(17), int64(1), object(1)\n",
      "memory usage: 4.2+ MB\n",
      "None\n",
      "         T_data_1_1    T_data_1_2    T_data_1_3    T_data_2_1    T_data_2_2  \\\n",
      "count  29184.000000  29184.000000  29184.000000  29184.000000  29184.000000   \n",
      "mean     250.033802    249.807266    250.129786    349.786073    349.750866   \n",
      "std       31.671135     29.885764     30.048873     42.272453     40.046864   \n",
      "min     -185.850000   -115.566667    -95.200000   -655.616667   -894.633333   \n",
      "25%      228.600000    228.583333    228.900000    328.466667    328.100000   \n",
      "50%      249.825000    249.766667    249.916667    350.033333    349.516667   \n",
      "75%      271.516667    271.237500    271.433333    371.804167    371.883333   \n",
      "max      711.566667    561.483333    647.450000   1278.683333   1135.183333   \n",
      "\n",
      "         T_data_2_3    T_data_3_1    T_data_3_2    T_data_3_3    T_data_4_1  \\\n",
      "count  29184.000000  29184.000000  29184.000000  29184.000000  29184.000000   \n",
      "mean     349.978356    501.180280    500.891807    501.286964    348.942375   \n",
      "std       36.683061     60.843930     62.881778     59.814458     37.507977   \n",
      "min     -141.966667   -593.350000   -724.416667   -264.250000   -451.866667   \n",
      "25%      328.595833    463.833333    463.516667    464.183333    326.900000   \n",
      "50%      349.950000    501.833333    501.900000    502.050000    348.866667   \n",
      "75%      371.600000    537.466667    537.000000    538.270833    370.866667   \n",
      "max      858.250000   1544.983333   2459.200000   1224.416667   1148.450000   \n",
      "\n",
      "         T_data_4_2    T_data_4_3    T_data_5_1    T_data_5_2    T_data_5_3  \\\n",
      "count  29184.000000  29184.000000  29184.000000  29184.000000  29184.000000   \n",
      "mean     348.979526    349.660812    249.822262    249.655515    249.830805   \n",
      "std       38.936543     38.499445     30.119489     29.990314     30.048957   \n",
      "min     -447.250000   -596.966667    -57.233333   -117.566667    -92.566667   \n",
      "25%      327.366667    327.783333    228.683333    228.733333    228.733333   \n",
      "50%      348.933333    349.458333    249.600000    249.766667    249.816667   \n",
      "75%      370.816667    371.383333    270.366667    270.816667    270.600000   \n",
      "max     1211.850000    877.950000    859.716667    638.383333    618.383333   \n",
      "\n",
      "             H_data       AH_data       quality  \n",
      "count  29184.000000  29184.000000  29184.000000  \n",
      "mean     174.723367      7.500540    402.800747  \n",
      "std       13.794259      1.146928     46.273228  \n",
      "min      149.357000      3.120000    221.000000  \n",
      "25%      163.160292      6.730000    372.000000  \n",
      "50%      174.367917      7.510000    408.000000  \n",
      "75%      186.220250      8.280000    439.000000  \n",
      "max      200.660000     11.620000    505.000000  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29184 entries, 0 to 29183\n",
      "Data columns (total 19 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   date_time   29184 non-null  datetime64[ns]\n",
      " 1   T_data_1_1  29184 non-null  float64       \n",
      " 2   T_data_1_2  29184 non-null  float64       \n",
      " 3   T_data_1_3  29184 non-null  float64       \n",
      " 4   T_data_2_1  29184 non-null  float64       \n",
      " 5   T_data_2_2  29184 non-null  float64       \n",
      " 6   T_data_2_3  29184 non-null  float64       \n",
      " 7   T_data_3_1  29184 non-null  float64       \n",
      " 8   T_data_3_2  29184 non-null  float64       \n",
      " 9   T_data_3_3  29184 non-null  float64       \n",
      " 10  T_data_4_1  29184 non-null  float64       \n",
      " 11  T_data_4_2  29184 non-null  float64       \n",
      " 12  T_data_4_3  29184 non-null  float64       \n",
      " 13  T_data_5_1  29184 non-null  float64       \n",
      " 14  T_data_5_2  29184 non-null  float64       \n",
      " 15  T_data_5_3  29184 non-null  float64       \n",
      " 16  H_data      29184 non-null  float64       \n",
      " 17  AH_data     29184 non-null  float64       \n",
      " 18  quality     29184 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(17), int64(1)\n",
      "memory usage: 4.2 MB\n",
      "None\n",
      "            date_time  T_data_1_1  T_data_1_2  T_data_1_3  T_data_2_1  \\\n",
      "0 2015-01-04 00:05:00  271.745455  338.545455  267.218182  328.236364   \n",
      "1 2015-01-04 01:05:00  277.583333  300.366667  273.550000  320.483333   \n",
      "\n",
      "   T_data_2_2  T_data_2_3  T_data_3_1  T_data_3_2  T_data_3_3  T_data_4_1  \\\n",
      "0  329.400000  345.472727  501.981818      499.40  588.636364  320.490909   \n",
      "1  331.966667  355.450000  501.900000      501.65  705.516667  330.233333   \n",
      "\n",
      "   T_data_4_2  T_data_4_3  T_data_5_1  T_data_5_2  T_data_5_3    H_data  \\\n",
      "0  362.181818  336.363636  232.236364  236.454545  242.454545  155.9000   \n",
      "1  387.133333  336.316667  231.850000  237.683333  236.516667  156.4465   \n",
      "\n",
      "   AH_data  quality  year  month  day  hour  \n",
      "0     4.73      392  2015      1    4     0  \n",
      "1     7.90      384  2015      1    4     1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29184 entries, 0 to 29183\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   date_time      29184 non-null  datetime64[ns]\n",
      " 1   T_data_1_1     29184 non-null  float64       \n",
      " 2   T_data_1_2     29184 non-null  float64       \n",
      " 3   T_data_1_3     29184 non-null  float64       \n",
      " 4   T_data_2_1     29184 non-null  float64       \n",
      " 5   T_data_2_2     29184 non-null  float64       \n",
      " 6   T_data_2_3     29184 non-null  float64       \n",
      " 7   T_data_3_1     29184 non-null  float64       \n",
      " 8   T_data_3_2     29184 non-null  float64       \n",
      " 9   T_data_3_3     29184 non-null  float64       \n",
      " 10  T_data_4_1     29184 non-null  float64       \n",
      " 11  T_data_4_2     29184 non-null  float64       \n",
      " 12  T_data_4_3     29184 non-null  float64       \n",
      " 13  T_data_5_1     29184 non-null  float64       \n",
      " 14  T_data_5_2     29184 non-null  float64       \n",
      " 15  T_data_5_3     29184 non-null  float64       \n",
      " 16  H_data         29184 non-null  float64       \n",
      " 17  AH_data        29184 non-null  float64       \n",
      " 18  year           29184 non-null  int32         \n",
      " 19  month          29184 non-null  int32         \n",
      " 20  day            29184 non-null  int32         \n",
      " 21  hour           29184 non-null  int32         \n",
      " 22  quality_class  29184 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(17), int32(4), int64(1)\n",
      "memory usage: 4.7 MB\n",
      "None\n",
      "            date_time  T_data_1_1  T_data_1_2  T_data_1_3  T_data_2_1  \\\n",
      "0 2015-01-04 00:05:00  271.745455  338.545455  267.218182  328.236364   \n",
      "1 2015-01-04 01:05:00  277.583333  300.366667  273.550000  320.483333   \n",
      "\n",
      "   T_data_2_2  T_data_2_3  T_data_3_1  T_data_3_2  T_data_3_3  T_data_4_1  \\\n",
      "0  329.400000  345.472727  501.981818      499.40  588.636364  320.490909   \n",
      "1  331.966667  355.450000  501.900000      501.65  705.516667  330.233333   \n",
      "\n",
      "   T_data_4_2  T_data_4_3  T_data_5_1  T_data_5_2  T_data_5_3    H_data  \\\n",
      "0  362.181818  336.363636  232.236364  236.454545  242.454545  155.9000   \n",
      "1  387.133333  336.316667  231.850000  237.683333  236.516667  156.4465   \n",
      "\n",
      "   AH_data  year  month  day  hour  quality_class  \n",
      "0     4.73  2015      1    4     0              0  \n",
      "1     7.90  2015      1    4     1              0  \n",
      "(29184, 21)\n",
      "(29184,)\n",
      "False\n",
      "False\n",
      "quality_class\n",
      "0    14886\n",
      "1    14298\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We import all relevant packages required for modeling with RF, AdaBoost, LR & SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # We filter out warnings that specific features will be discontinued in future\n",
    "\n",
    "# We load the merged dataset\n",
    "dataset = pd.read_csv(\"/content/data_merged_mean.csv\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(dataset.head())\n",
    "print(dataset.shape)\n",
    "print(dataset.info())\n",
    "print(dataset.describe())\n",
    "\n",
    "# We handle date-time\n",
    "# We extract useful features from it, in case the time, day or date could be important for prediction\n",
    "dataset['date_time'] = pd.to_datetime(dataset['date_time'])\n",
    "print(dataset.info())\n",
    "final_data = dataset\n",
    "final_data['year'] = final_data['date_time'].dt.year\n",
    "final_data['month'] = final_data['date_time'].dt.month\n",
    "final_data['day'] = final_data['date_time'].dt.day\n",
    "final_data['hour'] = final_data['date_time'].dt.hour\n",
    "print(final_data.head(2))\n",
    "\n",
    "# We redefine 'quality' as a categorical variable so that it can be used for binary classification\n",
    "# Here the quality is set to positive class (high quality) if it is greater than or equal to 410\n",
    "# This is to ensure that we eliminate false negatives (low quality products that are predicted as high quality) by maximizing PRECISION\n",
    "final_data['quality_class'] = np.where(final_data['quality'] >= 410, 1, 0)\n",
    "\n",
    "# # Correlation matrix\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# corr_matrix = final_data.corr()\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "# plt.title('Correlation Matrix of Features')\n",
    "# plt.show()\n",
    "\n",
    "# We drop the old 'quality' column containing numerical quality measurements\n",
    "final_data = final_data.drop('quality', axis=1)\n",
    "\n",
    "print(final_data.info())\n",
    "print(final_data.head(2))\n",
    "\n",
    "# We redefine X and Y\n",
    "X = final_data.drop(['quality_class', 'date_time'], axis=1)\n",
    "Y = final_data['quality_class']\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X)\n",
    "\n",
    "print(np.any(np.isnan(X_scaled)))  # We check for NaNs\n",
    "print(np.any(np.isinf(X_scaled)))  # We check for infinite values\n",
    "\n",
    "print(Y.value_counts()) # We count the number of high & low quality instances for cost-benefit analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yNx2VYsB6U7"
   },
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iZ-qINN9IIi"
   },
   "source": [
    "### Random Forest with ALL Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTke7ETsqiBP",
    "outputId": "cd79e5ab-0424-423e-8d2e-cd0f0029ff17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification__n_estimators': 10}\n",
      "Precision:  0.9334954621569473\n",
      "Recall: 0.9947545111204364\n",
      "T_data_3_3    0.302721\n",
      "T_data_3_2    0.201301\n",
      "T_data_3_1    0.143297\n",
      "H_data        0.050298\n",
      "T_data_5_2    0.036001\n",
      "T_data_5_1    0.032217\n",
      "T_data_1_2    0.028095\n",
      "T_data_1_1    0.027054\n",
      "T_data_5_3    0.026127\n",
      "T_data_1_3    0.023558\n",
      "T_data_2_3    0.019436\n",
      "T_data_2_2    0.018442\n",
      "T_data_2_1    0.015259\n",
      "AH_data       0.012924\n",
      "T_data_4_3    0.011590\n",
      "T_data_4_2    0.011427\n",
      "T_data_4_1    0.010681\n",
      "day           0.009308\n",
      "hour          0.008735\n",
      "month         0.007713\n",
      "year          0.003817\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# We implement the Random Forest model beginning with no. of estimators: [50,100,200,500,800]\n",
    "# This is then tuned repeatedly until we get the best parameter\n",
    "\n",
    "model = Pipeline([\n",
    "        ('balancing', SMOTE(random_state = 101)),\n",
    "        ('classification', RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=1) )\n",
    "    ])\n",
    "grid_param = {'classification__n_estimators': [7,9,10,11,13]}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='precision', cv=5)\n",
    "\n",
    "gd_sr.fit(X_scaled, Y)\n",
    "\n",
    "# We save the best parameters and results for comparison\n",
    "\n",
    "best_parameters_rf = gd_sr.best_params_\n",
    "print(best_parameters_rf)\n",
    "\n",
    "best_result_rf = gd_sr.best_score_\n",
    "print(\"Precision: \", best_result_rf)\n",
    "\n",
    "# We calculate and print recall score for the best model (for cost-benefit analysis)\n",
    "best_model = gd_sr.best_estimator_  # Get the best model from GridSearchCV\n",
    "Y_pred = best_model.predict(X_scaled)  # Make predictions on the full dataset\n",
    "recall = recall_score(Y, Y_pred)  # Calculate recall score\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# We print the feature importance list to run new models with the top features\n",
    "\n",
    "featimp = pd.Series(gd_sr.best_estimator_.named_steps[\"classification\"].feature_importances_, index=list(X)).sort_values(ascending=False) # Getting feature importance list for the best model\n",
    "print(featimp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnDneTgW9N8d"
   },
   "source": [
    "### Random Forest with TOP 3 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0orvz7elOGX",
    "outputId": "1ec83227-550f-4dc9-b75f-752d72cd9325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification__n_estimators': 10}\n",
      "Precision:  0.8605403366630145\n"
     ]
    }
   ],
   "source": [
    "# The top 3 features as given by RF are temperature data for chamber 3\n",
    "# We select features with higher significance and redefine the feature set\n",
    "X_3 = final_data[['T_data_3_2', 'T_data_3_3', 'T_data_3_1']]\n",
    "\n",
    "# We scale the data so that it is all given equal weightage\n",
    "feature_scaler = StandardScaler()\n",
    "X_scaled_3 = feature_scaler.fit_transform(X_3)\n",
    "\n",
    "# We tune the random forest parameter 'n_estimators' beginnig from [10,50,100,200,300] and refine repeatedly\n",
    "# We also implement cross-validation using Grid Search\n",
    "model = Pipeline([\n",
    "        ('balancing', SMOTE(random_state = 101)),\n",
    "        ('classification', RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=1) )\n",
    "    ])\n",
    "grid_param = {'classification__n_estimators': [7,9,10,11,13]}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='precision', cv=5)\n",
    "\n",
    "gd_sr.fit(X_scaled_3, Y)\n",
    "\n",
    "# We save the best parameters and results for comparison\n",
    "\n",
    "best_parameters_rf3 = gd_sr.best_params_\n",
    "print(best_parameters_rf3)\n",
    "\n",
    "best_result_rf3 = gd_sr.best_score_\n",
    "print(\"Precision: \", best_result_rf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4YW162s9WE0"
   },
   "source": [
    "### Random Forest with TOP 4 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6YHq7ri9YLV",
    "outputId": "16eef8c3-c46a-46e5-c2d5-5cb226a18f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification__n_estimators': 10}\n",
      "Precision:  0.8785821364753211\n"
     ]
    }
   ],
   "source": [
    "# The top 4 features as given by RF are temperature data for chamber 3, and the height of input product\n",
    "# We select features with higher significance and redefine the feature set\n",
    "X_4 = final_data[['T_data_3_2', 'T_data_3_3', 'T_data_3_1', 'H_data']]\n",
    "\n",
    "# We scale the data so that it is all given equal weightage\n",
    "feature_scaler = StandardScaler()\n",
    "X_scaled_4 = feature_scaler.fit_transform(X_4)\n",
    "\n",
    "# We tune the random forest parameter 'n_estimators' beginnig from [10,50,100,200,300] and refine repeatedly\n",
    "# We also implement cross-validation using Grid Search\n",
    "model = Pipeline([\n",
    "        ('balancing', SMOTE(random_state = 101)),\n",
    "        ('classification', RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=1) )\n",
    "    ])\n",
    "grid_param = {'classification__n_estimators': [7,9,10,11,13]}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='precision', cv=5)\n",
    "\n",
    "gd_sr.fit(X_scaled_4, Y)\n",
    "\n",
    "# We save the best parameters and results for comparison\n",
    "\n",
    "best_parameters_rf4 = gd_sr.best_params_\n",
    "print(best_parameters_rf4)\n",
    "\n",
    "best_result_rf4 = gd_sr.best_score_\n",
    "print(\"Precision: \", best_result_rf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0DSY6m8qrlp"
   },
   "source": [
    "## ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJs6oaDkqvfx",
    "outputId": "bba3b78d-4166-4c48-cd19-e2581bedbf7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification__n_estimators': 250}\n",
      "Precision:  0.9217329846977884\n",
      "T_data_3_3    0.124\n",
      "T_data_3_2    0.120\n",
      "T_data_3_1    0.112\n",
      "H_data        0.072\n",
      "T_data_5_1    0.068\n",
      "T_data_1_2    0.068\n",
      "T_data_2_3    0.060\n",
      "T_data_1_1    0.056\n",
      "T_data_5_3    0.052\n",
      "T_data_5_2    0.048\n",
      "T_data_2_1    0.040\n",
      "T_data_4_2    0.028\n",
      "T_data_2_2    0.028\n",
      "T_data_1_3    0.024\n",
      "T_data_4_3    0.020\n",
      "day           0.020\n",
      "T_data_4_1    0.016\n",
      "AH_data       0.016\n",
      "hour          0.016\n",
      "month         0.012\n",
      "year          0.000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# We implement the Random Forest model beginning with no. of estimators: [50,100,200,500,800]\n",
    "# This is then tuned repeatedly until we get the best parameter\n",
    "\n",
    "model = Pipeline([\n",
    "        ('balancing', SMOTE(random_state = 101)),\n",
    "        ('classification', AdaBoostClassifier(random_state=1))\n",
    "    ])\n",
    "grid_param = {'classification__n_estimators': [240,250,260,270]}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='precision', cv=5)\n",
    "\n",
    "gd_sr.fit(X_scaled, Y)\n",
    "\n",
    "# We save the best parameters and results for comparison\n",
    "\n",
    "best_parameters_adb = gd_sr.best_params_\n",
    "print(best_parameters_adb)\n",
    "\n",
    "best_result_adb = gd_sr.best_score_\n",
    "print(\"Precision: \", best_result_adb)\n",
    "\n",
    "# We print the feature importance list to run new models with the top features\n",
    "\n",
    "featimp = pd.Series(gd_sr.best_estimator_.named_steps[\"classification\"].feature_importances_, index=list(X)).sort_values(ascending=False) # Getting feature importance list for the best model\n",
    "print(featimp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zElLp7dxB3S4"
   },
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_AX8F2mq7Xq",
    "outputId": "8c2ea396-b0e4-4ba4-fdc2-1a0816198385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification__alpha': 0.0001, 'classification__eta0': 1e-06, 'classification__l1_ratio': 1, 'classification__max_iter': 30}\n",
      "Precision:  0.9192557053152702\n"
     ]
    }
   ],
   "source": [
    "# We implement Logistic Regression using a Stochastic Gradient Descent Classifier\n",
    "# We tune eta0 to control the rate at which the model learns - a lower learning rate implies smaller increments/steps\n",
    "# We tune max_iter to set the upper limit on how many times the model passes over the training data to reach convergence\n",
    "# We tune alpha to control the strictness of regularization, i.e. the amount of penalty applied to larger coefficients to prevent overfitting\n",
    "# We tune the l1 ratio in elasticnet to control the proportion of lasso (L1) vs. ridge (L2) regularization applied\n",
    "# We implement cross-validation using Grid Search\n",
    "model = Pipeline([\n",
    "        ('balancing', SMOTE(random_state = 101)),\n",
    "        ('classification', SGDClassifier(loss = 'log_loss', penalty = 'elasticnet', random_state = 1))\n",
    "    ])\n",
    "grid_param = {'classification__eta0': [.000001,.000003,.000005,.000007,.00001], 'classification__max_iter' : [20,25,30,35], 'classification__alpha': [.0005,.0001,.005,.001], 'classification__l1_ratio': [0.1,0.5,0.7,0.9,1]}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='precision', cv=5)\n",
    "\n",
    "gd_sr.fit(X_scaled, Y)\n",
    "\n",
    "# We save the best parameters and results for comparison\n",
    "\n",
    "best_parameters_lr = gd_sr.best_params_\n",
    "print(best_parameters_lr)\n",
    "\n",
    "best_result_lr = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\n",
    "print(\"Precision: \", best_result_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iuz1BqNB9vf"
   },
   "source": [
    "## SUPPORT VECTOR CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5LXClN5B_G4"
   },
   "outputs": [],
   "source": [
    "# We tune the kernel to map the datapoints to the best-fitting higher dimension space\n",
    "# We tune the regularization parameter C to allow for wider/narrower margin\n",
    "# Larger C implies less regularization, i.e. a narrower margin and stricter classification, which may lead to a more complex model with overfitting\n",
    "\n",
    "model = Pipeline([\n",
    "        ('balancing', SMOTE(random_state = 101)),\n",
    "        ('classification', SVC(random_state=1) )\n",
    "    ])\n",
    "grid_param = {'classification__kernel': ['linear','poly','rbf','sigmoid'], 'classification__C': [.001,.01,.05,.01]}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='precision', cv=5)\n",
    "\n",
    "gd_sr.fit(X_scaled, Y)\n",
    "\n",
    "# We save the best parameters and results for comparison\n",
    "\n",
    "best_parameters_svc = gd_sr.best_params_\n",
    "print(best_parameters_svc)\n",
    "\n",
    "best_result_svc = gd_sr.best_score_\n",
    "print(\"Precision: \", best_result_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjpiBI713bVZ"
   },
   "source": [
    "## RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4YFZ8sC3eWp"
   },
   "outputs": [],
   "source": [
    "print(\"Precision Random Forest:       \", best_result_rf)\n",
    "print(\"Precision Random Forest Top 3: \", best_result_rf3)\n",
    "print(\"Precision Random Forest Top 4: \", best_result_rf4)\n",
    "print(\"Precision AdaBoost:            \", best_result_adb)\n",
    "print(\"Precision Logistic Regression: \", best_result_lr)\n",
    "print(\"Precision Support Vector:      \", best_result_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fTDYX7Gbuph"
   },
   "source": [
    "Since the product is a premium/high-end food product, we aim to minimize false positives. This is because a low quality product that is mistakenly sent into market will damage the reputation of the brand. This is unacceptable since quality cannot be compromised in a premium product, and the ripple effects among the customer base will be catastrophic.\n",
    "Comparatively, sacrificing a proportion of mis-classified high quality products is not as big an issue, since only the cost of production + QC + opportunity cost is lost. This is a small price to pay to preserve brand value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leGQs-1adZOZ"
   },
   "source": [
    "The best precision is obtained from a Random Forest Classifier utilizing all the features (columns).\n",
    "We observe the precision reducing significantly when choosing just the top features. Hence, all the input values are of some importance in prediction, despite the top three features (temperatures of chamber 3) having relatively a far greater impact."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
